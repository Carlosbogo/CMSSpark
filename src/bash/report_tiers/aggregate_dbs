#!/bin/sh

if [ "$1" == "-h" ] || [ "$1" == "-help" ] || [ "$1" == "--help" ]; then
    echo "Usage: ./"$(basename "$0")" [location]"
    echo "Example: ./"$(basename "$0")" /cms/users/$USER/dbs_datasets"
    echo "[location] = hdfs location of resulting files. Default: /cms/users/$USER/dbs_datasets"
    exit 0
fi

location=/cms/users/$USER/dbs_datasets

if [[ -n $1 ]]
then
    location=$1
fi

echo 'Results path: '$location

# Results will be put here
hdir=hdfs://$location

# Get current script's absolute path
scriptpath="$( cd "$(dirname "$0")" ; pwd -P )"

# Remove previous data first
hadoop fs -rm -r -skipTrash $location

PYTHONPATH=$scriptpath/../../python $scriptpath/../../../bin/run_spark /reports/aggregate_dbs.py --fout=$hdir --yarn --verbose

hadoop fs -test -e $hdir
exists=$?

# Download results and recreate csv files only if results exist in hdfs
if [[ $exists -eq 0 ]]
then
    result_dir="$(basename $hdir)"

    # Delete previously downloaded directory and download new one
    rm -rf "$scriptpath/$result_dir"
    hadoop fs -get $hdir $scriptpath

    # extract header
    head -1 $scriptpath/$result_dir/part-00000 > $scriptpath/dbs_df.csv

    # concatenate all parts except header
    header=`cat $scriptpath/dbs_df.csv`
    cat $scriptpath/$result_dir/part* | grep -v $header >> $scriptpath/dbs_df.csv
fi
