{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    div.output_stderr {\n",
    "    display: none;\n",
    "}\n",
    "</style>\n",
    "# To disable Spark warnings in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c353ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Author: Ceyhun Uzunoglu <ceyhunuzngl AT gmail [DOT] com>\n",
    "\"\"\"Get last access times of datasets by joining Rucio's REPLICAS, DIDS and CONTENTS tables\"\"\"\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg as _avg,\n",
    "    col,\n",
    "    count as _count,\n",
    "    countDistinct,\n",
    "    hex as _hex,\n",
    "    lit,\n",
    "    lower,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    "    when,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    ")\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "#TODAY = \"2022-01-13\"\n",
    "TODAY = datetime.today().strftime('%Y-%m-%d')\n",
    "HDFS_RUCIO_CONTENTS = f\"/project/awg/cms/rucio_contents/{TODAY}/part*.avro\"\n",
    "HDFS_RUCIO_DIDS = f\"/project/awg/cms/rucio_dids/{TODAY}/part*.avro\"\n",
    "HDFS_RUCIO_REPLICAS = f\"/project/awg/cms/rucio/{TODAY}/replicas/part*.avro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f509d4",
   "metadata": {},
   "source": [
    "########################################################\n",
    "\n",
    "    ---- Assumptions and explanations of Rucio tables ----\n",
    "     --- DBS vs Rucio terminology ---\n",
    "       - file:    [F]ile in Rucio\n",
    "       - block:   [D]ataset in Rucio\n",
    "       - dataset: [C]ontainer in Rucio\n",
    "\n",
    "    NOT: We used DBS terminology otherwise specified implicitly!!!\n",
    "    \n",
    "    Mainly:\n",
    "        ACCESSED_AT (last access time)                     : comes from DIDS\n",
    "        BYTES (file size)                                  : combined values of DIDS and REPLICAS\n",
    "        RSE ID - FILE relation                             : comes from REPLICAS\n",
    "        All file-block, block-dataset membership/ownership : comes from CONTENTS\n",
    "\n",
    "    Process:\n",
    "        - Basically joining dataframes of Rucio tables to get: Dataset, Block, File, RseId, Access time, Size\n",
    "        - In function&dataframe names: f, b, d are initial letters of file, block and dataset\n",
    "        - Get access times of files\n",
    "        - Get rse ids: filter only Disk Rses\n",
    "        - Get blocks of files\n",
    "        - Get datasets of blocks\n",
    "        - Filter datasets in which all files of it contains access time; drop others\n",
    "        - Calculate Dataset's last access time by getting max of its file access times\n",
    "        - Group by datasets, rses to differentiate size and last access time of datasets in rses\n",
    "        - Group access times of Dataset if they are accessed at least 12, 6, 3 months ago\n",
    "        - Print total sizes of Datasets with their last access time groups\n",
    "\n",
    "    --- Rucio table usages: ---\n",
    "    1. CMS_RUCIO_PROD.CONTENTS\n",
    "        Includes file dataset block relationships in only one degree.\n",
    "        Provides all files that a dataset contains, or all datasets that a file belongs to\n",
    "        Provides all datasets that a container contains, or all containers that a dataset belongs to\n",
    "        DID_TYPE-CHILD_TYPE can be:  D-F or C-D (Rucio terminology first letters)\n",
    "        Used columns: SCOPE, NAME, CHILD_SCOPE, CHILD_NAME, DID_TYPE, CHILD_TYPE\n",
    "    \n",
    "    2. CMS_RUCIO_PROD.DIDS\n",
    "        Provides all files in CMS scope\n",
    "        Provides file last access time\n",
    "        Provides file size (compatibility with Rucio REPLICAS table is checked and they provide same values)\n",
    "        Only files are selected: DID_TYPE=F\n",
    "        Used columns: SCOPE, DID_TYPE, NAME, ACCESSED_AT, BYTES\n",
    "    \n",
    "    3. CMS_RUCIO_PROD.REPLICAS\n",
    "        Provides files that exist in Disk RSEs, in CMS scope.\n",
    "        Provides file sizes in in Disk RSEs (same with DIDS)\n",
    "        Only Disk RSE_IDs are selected\n",
    "        Used columns: SCOPE, RSE_ID BYTES NAME\n",
    "        \n",
    "Reference\n",
    " - Sqoop jobs that dumps Rucio tables to hdfs: https://github.com/dmwm/CMSKubernetes/tree/master/docker/sqoop/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_months_ago_epoch_msec(n_months_ago):\n",
    "    \"\"\"Returns integer unix timestamp msec of n months ago from today\"\"\"\n",
    "    dt = datetime.today() + relativedelta(months=-n_months_ago)  # minus\n",
    "    return int(datetime(dt.year, dt.month, dt.day).timestamp()) * 1000\n",
    "\n",
    "\n",
    "def get_rse_ids():\n",
    "    \"\"\"Get rse:rse_id map from pickle file and return rse_ids as list\n",
    "\n",
    "    See CMSMONIT-324 how to fetch only Disk RSE_IDs using Rucio cli (author: David Lange)\n",
    "    \"\"\"\n",
    "    with open(\"rses.pickle\", \"rb+\") as f:\n",
    "        rses = pickle.load(f)\n",
    "    return list(\n",
    "        dict(\n",
    "            [(k, v) for k, v in rses.items() if not any(tmp in k for tmp in [\"Tape\", \"Test\", \"Temp\"])]\n",
    "        ).values()\n",
    "    )\n",
    "\n",
    "\n",
    "# def get_spark_session(yarn=True, verbose=False):\n",
    "#     \"\"\"Get or create the spark context and session.\n",
    "#     \"\"\"\n",
    "#     sc = SparkContext(appName=\"cms-monitoring-rucio-last_access-ts\")\n",
    "#     return SparkSession.builder.config(conf=sc._conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b275790",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "# Prepare Spark dataframes in separate functions\n",
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_replicas(spark, disk_rse_ids):\n",
    "    \"\"\"Create main replicas dataframe by selecting only Disk RSEs\n",
    "\n",
    "    Columns selected:\n",
    "        - file: file name\n",
    "        - fsize_replicas: represents size of a file in REPLICAS table\n",
    "        - rse_id\n",
    "\n",
    "    df_replicas: Main replicas Spark dataframe for this script\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"avro\").load(HDFS_RUCIO_REPLICAS) \\\n",
    "        .withColumn(\"rse_id\", lower(_hex(col(\"RSE_ID\")))) \\\n",
    "        .withColumn(\"fsize_replicas\", col(\"BYTES\").cast(LongType())) \\\n",
    "        .withColumnRenamed(\"NAME\", \"file\") \\\n",
    "        .filter(col(\"rse_id\").isin(disk_rse_ids)) \\\n",
    "        .filter(col(\"SCOPE\") == \"cms\") \\\n",
    "        .select([\"file\", \"rse_id\", \"fsize_replicas\"]) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_dids_files(spark):\n",
    "    \"\"\"Create spark dataframe for DIDS table by selecting only Files.\n",
    "\n",
    "    Filters:\n",
    "        - DELETED_AT not null\n",
    "        - HIDDEN = 0\n",
    "        - SCOPE = cms\n",
    "        - DID_TYPE = F\n",
    "\n",
    "    Columns selected:\n",
    "        - file: file name\n",
    "        - fsize_dids: represents size of a file in DIDS table\n",
    "        - accessed_at: file last access time\n",
    "\n",
    "    df_dids_files: All files catalog, their sizes and last access times\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"avro\").load(HDFS_RUCIO_DIDS) \\\n",
    "        .filter(col(\"DELETED_AT\").isNull()) \\\n",
    "        .filter(col(\"HIDDEN\") == \"0\") \\\n",
    "        .filter(col(\"SCOPE\") == \"cms\") \\\n",
    "        .filter(col(\"DID_TYPE\") == \"F\") \\\n",
    "        .withColumnRenamed(\"NAME\", \"file\") \\\n",
    "        .withColumnRenamed(\"ACCESSED_AT\", \"accessed_at\") \\\n",
    "        .withColumn(\"fsize_dids\", col(\"BYTES\").cast(LongType())) \\\n",
    "        .select([\"file\", \"fsize_dids\", \"accessed_at\"]) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_replicas_j_dids(df_replicas, df_dids_files):\n",
    "    \"\"\"Left join of df_replicas and df_dids_files to fill the RSE_ID and fsize for all files.\n",
    "\n",
    "    Be aware that there are 2 file size columns, they will be combined in \"df_f_rse_ts_size\".\n",
    "\n",
    "    Columns:\n",
    "        comes from DID:       file, accessed_at, fsize_dids,\n",
    "        comes from REPLICAS:  file, rse_id, fsize_replicas\n",
    "\n",
    "    df_replicas_j_dids: Filled fsize for all files by combining both REPLICAS values and DIDS values\n",
    "   \"\"\"\n",
    "    return df_replicas.join(df_dids_files, [\"file\"], how=\"left\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa20aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_replicas_dids_join_is_desired(df_replicas_j_dids):\n",
    "    \"\"\"Check all files have size values in joined dataframe and also 2 tables' size values are equal for same files\n",
    "\n",
    "    df_replicas_j_dids is left join of REPLICAS and DIDS\n",
    "    There are 2 tables to get file sizes: REPLICAS, DIDS.\n",
    "    Not all files have size information, so we'll use combined values of above tables to set sizes of all files.\n",
    "    Then check:\n",
    "        1. All files have size information\n",
    "        2. Size values of DIDS and REPLICAS tables are equal for same files\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that REPLICAS and DIDS join filled the size values of all files. Yes!\n",
    "    if df_replicas_j_dids.filter(\n",
    "        col(\"fsize_dids\").isNull() & col(\"fsize_replicas\").isNull()\n",
    "    ).head():\n",
    "        print(\"We have a problem! At least one file does not have size info!\")\n",
    "        return False\n",
    "    # Check that REPLICAS and DIDS size values are compatible. Yes!\n",
    "    elif df_replicas_j_dids.withColumn(\n",
    "        \"bytes_ratio\",\n",
    "        when(\n",
    "            col(\"fsize_dids\").isNotNull() & col(\"fsize_replicas\").isNotNull(),\n",
    "            col(\"fsize_dids\") / col(\"fsize_replicas\")\n",
    "        ).otherwise(\"0\")\n",
    "    ).filter(\n",
    "        (col(\"bytes_ratio\") != 1.0) & (col(\"bytes_ratio\") != 0.0)\n",
    "    ).head():\n",
    "        print(\"We have a problem, bytes are not equal in DIDS and REPLICAS!\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"df_replicas_j_dids is desired.\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_file_rse_ts_size(df_replicas_j_dids):\n",
    "    \"\"\"fsize_dids or fsize_replicas should not be null. Just combine them to fill file sizes.\n",
    "\n",
    "    Firstly, REPLICAS size value will be used. If there are files with no size values, DIDS size values will be used:\n",
    "    see \"when\" function order.\n",
    "\n",
    "    Columns: file, rse_id, accessed_at, fsize\n",
    "\n",
    "    df_f_rse_ts_size: files and their rse_id, size and access time are completed\n",
    "    \"\"\"\n",
    "    return df_replicas_j_dids.withColumn(\"fsize\",\n",
    "                                         when(col(\"fsize_replicas\").isNotNull(), col(\"fsize_replicas\"))\n",
    "                                         .when(col(\"fsize_dids\").isNotNull(), col(\"fsize_dids\"))\n",
    "                                         ) \\\n",
    "        .select(['file', 'rse_id', 'accessed_at', 'fsize']) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42592775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_contents_f_to_b(spark):\n",
    "    \"\"\"Get all files that a block contains, or all blocks that a file belongs to.\n",
    "\n",
    "    Columns selected: block, file\n",
    "\n",
    "    df_contents_f_to_b: FILE-BLOCK membership/ownership map\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"com.databricks.spark.avro\").load(HDFS_RUCIO_CONTENTS) \\\n",
    "        .filter(col(\"SCOPE\") == \"cms\") \\\n",
    "        .filter(col(\"DID_TYPE\") == \"D\") \\\n",
    "        .filter(col(\"CHILD_TYPE\") == \"F\") \\\n",
    "        .withColumnRenamed(\"NAME\", \"block\") \\\n",
    "        .withColumnRenamed(\"CHILD_NAME\", \"file\") \\\n",
    "        .select([\"block\", \"file\"]) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59889887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_b_f_rse_ts_size(df_f_rse_ts_size, df_contents_f_to_b):\n",
    "    \"\"\" Left join df_f_rse_ts_size and df_contents_f_to_b to get block names of files.\n",
    "\n",
    "    Columns: block(from df_contents_f_to_b), file, rse_id, accessed_at, fsize\n",
    "\n",
    "    df_b_f_rse_ts_size: add \"block\" names to \"df_f_rse_ts_size\" dataframe\n",
    "    \"\"\"\n",
    "    df_b_f_rse_ts_size = df_f_rse_ts_size \\\n",
    "        .join(df_contents_f_to_b, [\"file\"], how=\"left\") \\\n",
    "        .select(['block', 'file', 'rse_id', 'accessed_at', 'fsize']) \\\n",
    "        .cache()\n",
    "\n",
    "    print(\"Stats of df_b_f_rse_ts_size before filtering out null values =>\")\n",
    "    stats_of_df_b_f_rse_ts_size(df_b_f_rse_ts_size)\n",
    "\n",
    "    return df_b_f_rse_ts_size.filter(col(\"block\").isNotNull()).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_of_df_b_f_rse_ts_size(df_b_f_rse_ts_size):\n",
    "    \"\"\"Print statistics of df_b_f_rse_ts_size\n",
    "    \"\"\"\n",
    "    file_count = df_b_f_rse_ts_size.select(\"file\").count()\n",
    "    distinct_file_count = df_b_f_rse_ts_size.select(\"file\").distinct().count()\n",
    "    null_file_row_count = df_b_f_rse_ts_size.filter(col(\"file\").isNull()).count()\n",
    "    null_block_distinct_file_count = df_b_f_rse_ts_size.filter(col(\"block\").isNull()).select(\"file\").distinct().count()\n",
    "    null_accessed_at_distinct_file_count = \\\n",
    "        df_b_f_rse_ts_size.filter(col(\"accessed_at\").isNull()).select(\"file\").distinct().count()\n",
    "    null_block_count = df_b_f_rse_ts_size.filter(col(\"block\").isNull()).count()\n",
    "    null_rse_id_count = df_b_f_rse_ts_size.filter(col(\"rse_id\").isNull()).count()\n",
    "    null_fsize_count = df_b_f_rse_ts_size.filter(col(\"fsize\").isNull()).count()\n",
    "\n",
    "    print(\n",
    "        f\"Total file count: {file_count} \\n\",\n",
    "        f\"Total distinct file count: {distinct_file_count} \\n\",\n",
    "        f\"Null file row count: {null_file_row_count} \\n\",\n",
    "        f\"# of distinct files that have no block name: {null_block_distinct_file_count} \\n\",\n",
    "        f\"# of distinct files that have no accessed_at: {null_accessed_at_distinct_file_count} \\n\",\n",
    "        f\"Null block row count: {null_block_count} \\n\",\n",
    "        f\"Null RSE_ID row count: {null_rse_id_count} \\n\",\n",
    "        f\"Null file_size row count: {null_fsize_count} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27494369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_contents_b_to_d(spark):\n",
    "    \"\"\"Get all blocks that a dataset contains, or all datasets that a block belongs to.\n",
    "\n",
    "    Columns selected: dataset, block\n",
    "\n",
    "    df_contents_b_to_d: BLOCK-DATASET membership/ownership map\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"com.databricks.spark.avro\").load(HDFS_RUCIO_CONTENTS) \\\n",
    "        .filter(col(\"SCOPE\") == \"cms\") \\\n",
    "        .filter(col(\"DID_TYPE\") == \"C\") \\\n",
    "        .filter(col(\"CHILD_TYPE\") == \"D\") \\\n",
    "        .withColumnRenamed(\"NAME\", \"dataset\") \\\n",
    "        .withColumnRenamed(\"CHILD_NAME\", \"block\") \\\n",
    "        .select([\"dataset\", \"block\"]) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc13111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_d_b_f_rse_ts_size(df_b_f_rse_ts_size, df_contents_b_to_d):\n",
    "    \"\"\"Left join df_b_f_rse_ts_size and df_contents_b_to_d to get dataset names of blocks.\n",
    "\n",
    "    Columns: dataset(from df_contents_b_to_d), block, file, rse_id, accessed_at, fsize\n",
    "\n",
    "    df_d_b_f_rse_ts_size: add \"dataset\" name to df_b_f_rse_ts_size.\n",
    "    \"\"\"\n",
    "    df_d_b_f_rse_ts_size = df_b_f_rse_ts_size \\\n",
    "        .join(df_contents_b_to_d, [\"block\"], how=\"left\") \\\n",
    "        .select(['dataset', 'block', 'file', 'rse_id', 'accessed_at', 'fsize']) \\\n",
    "        .cache()\n",
    "\n",
    "    null_dataset_distinct_block_count = df_d_b_f_rse_ts_size.filter(\n",
    "        col(\"dataset\").isNull()\n",
    "    ).select(\"block\").distinct().count()\n",
    "\n",
    "    print(\"Stats of df_d_b_f_rse_ts_size before filtering out null values =>\")\n",
    "    print(f\"Number of Distinct blocks that has no dataset name: {null_dataset_distinct_block_count}\")\n",
    "\n",
    "    return df_d_b_f_rse_ts_size.filter(col(\"dataset\").isNotNull()).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48842661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_datasets_rses_group_by(df_d_b_f_rse_ts_size):\n",
    "    \"\"\"Group by \"dataset\" and \"rse_id\" of df_d_b_f_rse_ts_size\n",
    "\n",
    "    Calculations will produce below columns:\n",
    "        - \"dataset_size_in_rse\"\n",
    "                Total size of a Dataset in an RSE.\n",
    "                Produced by summing up datasets' all files in that RSE.\n",
    "        - \"last_access_time_of_dataset_in_rse\"\n",
    "                Last access time of a Dataset in an RSE.\n",
    "                Produced by getting max `accessed_at`(represents single file's access time) of a dataset in an RSE.\n",
    "        - \"#files_with_null_access_time_of_dataset_in_rse\"\n",
    "                Number of files count, which have NULL `accessed_at` values, of a Dataset in an RSE.\n",
    "                This is important to know to filter out if there is any NULL `accessed_at` value of a Dataset.\n",
    "        - \"#files_of_dataset_in_rse\"\n",
    "                Number of files count of a Dataset in an RSE\n",
    "        - \"#distinct_files_of_dataset_in_rse\"\n",
    "                Number of unique files count of dataset in an RSE\n",
    "\n",
    "        Final result will be like:\n",
    "            One dataset can be in multiple RSEs and\n",
    "            presumably it may have different sizes since a dataset may have lost some of its blocks or files in an RSE?\n",
    "\n",
    "    Columns: dataset, rse_id,\n",
    "             dataset_size_in_rse,\n",
    "             last_access_time_of_dataset_in_rse,\n",
    "             #files_with_null_access_time_of_dataset_in_rse,\n",
    "             #files_of_dataset_in_rse,\n",
    "             #distinct_files_of_dataset_in_rse\n",
    "\n",
    "\n",
    "    df_datasets_rses_group_by: dataset, rse_id and their size and access time calculations\n",
    "    \"\"\"\n",
    "    return df_d_b_f_rse_ts_size \\\n",
    "        .groupby([\"rse_id\", \"dataset\"]) \\\n",
    "        .agg(_sum(col(\"fsize\")).alias(\"dataset_size_in_rse\"),\n",
    "             _max(col(\"accessed_at\")).alias(\"last_access_time_of_dataset_in_rse\"),\n",
    "             _sum(\n",
    "                 when(col(\"accessed_at\").isNull(), 1).otherwise(0)\n",
    "             ).alias(\"#files_with_null_access_time_of_dataset_in_rse\"),\n",
    "             _count(lit(1)).alias(\"#files_of_dataset_in_rse\"),\n",
    "             countDistinct(col(\"file\")).alias(\"#distinct_files_of_dataset_in_rse\"),\n",
    "             ) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_of_null_accessed_at(df_datasets_rses_group_by):\n",
    "    \"\"\"Statistics of Datasets which have only null accessed_at fields in its files\n",
    "    \"\"\"\n",
    "    df_all_null_accessed_at = df_datasets_rses_group_by \\\n",
    "        .filter(col(\"last_access_time_of_dataset_in_rse\").isNull()) \\\n",
    "        .groupby([\"dataset\"]) \\\n",
    "        .agg(_round(_max(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"max_dataset_size_in_rses(TB)\"),\n",
    "             _round(_min(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"min_dataset_size_in_rses(TB)\"),\n",
    "             _round(_avg(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"avg_dataset_size_in_rses(TB)\"),\n",
    "             _sum(col(\"#files_with_null_access_time_of_dataset_in_rse\")\n",
    "                  ).alias(\"#files_with_null_access_time_per_dataset\"),\n",
    "             ) \\\n",
    "        .cache()\n",
    "    print(\"Stats of Datasets which only have NULL access time =>\")\n",
    "    df_all_null_accessed_at.select(\n",
    "        [\"max_dataset_size_in_rses(TB)\", \"min_dataset_size_in_rses(TB)\", \"avg_dataset_size_in_rses(TB)\"]\n",
    "    ).groupBy().sum().show()\n",
    "    print(\"Count of Datasets which only have NULL access time\",\n",
    "          df_all_null_accessed_at.select(\"dataset\").distinct().count())\n",
    "    del df_all_null_accessed_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58046362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_final(df_datasets_rses_group_by):\n",
    "    \"\"\"Implement required filtering and calculate last_accessed_at_least_{12|6|3}_months_ago columns.\n",
    "\n",
    "    Filters:\n",
    "        - If a dataset contains EVEN a single file with null accessed_at, filter out\n",
    "\n",
    "    Group by Dataset to get final result from all RSEs' datasets.\n",
    "      - max_dataset_size_in_rses(TB): max size of dataset in all RSEs that contain this Dataset\n",
    "      - min_dataset_size_in_rses(TB): min size of dataset in all RSEs that contain this Dataset\n",
    "      - avg_dataset_size_in_rses(TB): avg size of dataset in all RSEs that contain this Dataset\n",
    "      - last_access_time_of_dataset_in_all_rses: latest access time of dataset in all RSEs\n",
    "\n",
    "    Logic:\n",
    "        Access time filter logic in order:\n",
    "        - If \"last_access_time_of_dataset_in_all_rses\" is less than 12 months ago,\n",
    "            set \"is_accessed_at_least_12_months_ago\" columns as 1\n",
    "        - If \"last_access_time_of_dataset_in_all_rses\" is less than 6 months ago,\n",
    "            set \"is_accessed_at_least_6_months_ago\" column as 1\n",
    "        - If \"last_access_time_of_dataset_in_all_rses\" is less than 3 months ago,\n",
    "            set \"is_accessed_at_least_3_months_ago\" columns as 1\n",
    "\n",
    "    The final result includes only the Datasets whose last access time is older than 3 months.\n",
    "    \"\"\"\n",
    "    return df_datasets_rses_group_by \\\n",
    "        .filter((\n",
    "                    col(\"last_access_time_of_dataset_in_rse\").isNotNull()\n",
    "                ) & (\n",
    "                    col(\"#files_with_null_access_time_of_dataset_in_rse\") == 0)\n",
    "                ) \\\n",
    "        .groupby([\"dataset\"]) \\\n",
    "        .agg(_round(_max(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"max_dataset_size_in_rses(TB)\"),\n",
    "             _round(_min(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"min_dataset_size_in_rses(TB)\"),\n",
    "             _round(_avg(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"avg_dataset_size_in_rses(TB)\"),\n",
    "             _sum(col(\"#files_with_null_access_time_of_dataset_in_rse\")).alias(\"#files_null_access_time_per_dataset\"),\n",
    "             _max(col(\"last_access_time_of_dataset_in_rse\")).alias(\"last_access_time_of_dataset_in_all_rses\"),\n",
    "             ) \\\n",
    "        .withColumn('is_accessed_at_least_12_months_ago',\n",
    "                    when(\n",
    "                        col('last_access_time_of_dataset_in_all_rses') < get_n_months_ago_epoch_msec(12),\n",
    "                        1).otherwise(0)\n",
    "                    ) \\\n",
    "        .withColumn('is_accessed_at_least_6_months_ago',\n",
    "                    when(col('last_access_time_of_dataset_in_all_rses') < get_n_months_ago_epoch_msec(6),\n",
    "                         1).otherwise(0)\n",
    "                    ) \\\n",
    "        .withColumn('is_accessed_at_least_3_months_ago',\n",
    "                    when(col('last_access_time_of_dataset_in_all_rses') < get_n_months_ago_epoch_msec(3),\n",
    "                         1).otherwise(0)\n",
    "                    ) \\\n",
    "        .filter((col('is_accessed_at_least_12_months_ago') == 1) |\n",
    "                (col('is_accessed_at_least_6_months_ago') == 1) |\n",
    "                (col('is_accessed_at_least_3_months_ago') == 1)\n",
    "                ) \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b63180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_df_final(df):\n",
    "    \"\"\"Stats of Datasets which not accessed at least 3,6,12 months ago\n",
    "    \"\"\"\n",
    "\n",
    "    # 12\n",
    "    print(\"<====== Total sizes of Datasets which are not accessed at least 12 months ======>\")\n",
    "    print(\n",
    "        \"Dataset Count :\",\n",
    "        df.filter(col(\"is_accessed_at_least_12_months_ago\") == 1).distinct().count()\n",
    "    )\n",
    "    df.filter(col(\"is_accessed_at_least_12_months_ago\") == 1).select(\n",
    "        [\"max_dataset_size_in_rses(TB)\", \"min_dataset_size_in_rses(TB)\", \"avg_dataset_size_in_rses(TB)\"]\n",
    "    ).groupBy().sum().show()\n",
    "\n",
    "    # 6\n",
    "    print(\"<====== Total sizes of Datasets which are not accessed at least 6 months ======>\")\n",
    "    print(\n",
    "        \"Dataset Count :\",\n",
    "        df.filter(col(\"is_accessed_at_least_6_months_ago\") == 1).distinct().count()\n",
    "    )\n",
    "    df.filter(col(\"is_accessed_at_least_6_months_ago\") == 1).select(\n",
    "        [\"max_dataset_size_in_rses(TB)\", \"min_dataset_size_in_rses(TB)\", \"avg_dataset_size_in_rses(TB)\"]\n",
    "    ).groupBy().sum().show()\n",
    "\n",
    "    # 3\n",
    "    print(\"<====== Total sizes of Datasets which are not accessed at least 3 months ======>\")\n",
    "    print(\n",
    "        \"Dataset Count :\",\n",
    "        df.filter(col(\"is_accessed_at_least_3_months_ago\") == 1).distinct().count()\n",
    "    )\n",
    "    df.filter(col(\"is_accessed_at_least_3_months_ago\") == 1).select(\n",
    "        [\"max_dataset_size_in_rses(TB)\", \"min_dataset_size_in_rses(TB)\", \"avg_dataset_size_in_rses(TB)\"]\n",
    "    ).groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64909e73",
   "metadata": {},
   "source": [
    "# Start execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rse ids of only Disk RSEs\n",
    "disk_rse_ids = get_rse_ids()\n",
    "# spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replicas = get_df_replicas(spark, disk_rse_ids)\n",
    "df_replicas.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da470bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dids_files = get_df_dids_files(spark)\n",
    "df_dids_files.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17613c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replicas_j_dids = get_df_replicas_j_dids(df_replicas, df_dids_files)\n",
    "df_replicas_j_dids.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_replicas_dids_join_is_desired(df_replicas_j_dids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f_rse_ts_size = get_df_file_rse_ts_size(df_replicas_j_dids)\n",
    "df_f_rse_ts_size.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb01a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contents_f_to_b = get_df_contents_f_to_b(spark)\n",
    "df_contents_f_to_b.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8989d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_f_rse_ts_size = get_df_b_f_rse_ts_size(df_f_rse_ts_size, df_contents_f_to_b)\n",
    "df_b_f_rse_ts_size.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contents_b_to_d = get_df_contents_b_to_d(spark)\n",
    "df_contents_b_to_d.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d45d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_b_f_rse_ts_size = get_df_d_b_f_rse_ts_size(df_b_f_rse_ts_size, df_contents_b_to_d)\n",
    "df_d_b_f_rse_ts_size.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets_rses_group_by = get_df_datasets_rses_group_by(df_d_b_f_rse_ts_size)\n",
    "df_datasets_rses_group_by.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_of_null_accessed_at(df_datasets_rses_group_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = get_df_final(df_datasets_rses_group_by)\n",
    "df_final.limit(10).toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08714c03",
   "metadata": {},
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_final(df_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.0.1"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
