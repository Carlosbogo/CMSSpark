{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c03671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Author: Ceyhun Uzunoglu <ceyhunuzngl AT gmail [DOT] com>\n",
    "\"\"\"Get last access times of datasets by joining Rucio's REPLICAS, DIDS and CONTENTS tables\"\"\"\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg as _avg,\n",
    "    col,\n",
    "    count as _count,\n",
    "    countDistinct,\n",
    "    hex as _hex,\n",
    "    lit,\n",
    "    lower,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    "    when,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    ")\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "HDFS_RUCIO_CONTENTS = \"/project/awg/cms/rucio_contents/*/part*.avro\"\n",
    "HDFS_RUCIO_DIDS = \"/project/awg/cms/rucio_dids/*/part*.avro\"\n",
    "\n",
    "\n",
    "# HDFS_RUCIO_REPLICAS, see below function\n",
    "\n",
    "\n",
    "def get_hdfs_rucio_replicas_path(spark):\n",
    "    \"\"\"Get replicas hdfs folder of today or yesterday\"\"\"\n",
    "    today = f\"/project/awg/cms/rucio/{datetime.today().strftime('%Y-%m-%d')}/replicas/\"\n",
    "    yesterday = f\"/project/awg/cms/rucio/{(datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')}/replicas/\"\n",
    "    HDFS_RUCIO_REPLICAS_EXT = \"part*.avro\"\n",
    "    jvm = spark._jvm\n",
    "    jsc = spark._jsc\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jsc.hadoopConfiguration())\n",
    "    if fs.exists(jvm.org.apache.hadoop.fs.Path(today)):\n",
    "        print(today + \" exists\")\n",
    "        return today + HDFS_RUCIO_REPLICAS_EXT\n",
    "    elif fs.exists(jvm.org.apache.hadoop.fs.Path(yesterday)):\n",
    "        print(yesterday + \" exists\")\n",
    "        return yesterday + HDFS_RUCIO_REPLICAS_EXT\n",
    "    else:\n",
    "        print(yesterday + \" NOT exists\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def get_ts_thresholds():\n",
    "    \"\"\"Returns unix timestamps of 3, 6 and 12 months ago\"\"\"\n",
    "    timestamps = {}\n",
    "    for num_month in [3, 6, 12]:\n",
    "        dt = datetime.today() + relativedelta(months=-num_month)\n",
    "        timestamps[num_month] = int(datetime(dt.year, dt.month, dt.day).timestamp()) * 1000\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def get_disk_rse_ids():\n",
    "    \"\"\"Get rse:rse_id map from pickle file\n",
    "\n",
    "    TODO: Get rse:rse_id map via Rucio python library. I could not run Rucio python library unfortunately.\n",
    "\n",
    "\n",
    "    Used code in LxPlus (author: David Lange):\n",
    "    ```py\n",
    "#!/usr/bin/env python\n",
    "from subprocess import Popen,PIPE\n",
    "import os,sys,pickle\n",
    "def runCommand(comm):\n",
    "    p = Popen(comm,stdout=PIPE,stderr=PIPE,shell=True)\n",
    "    pipe=p.stdout.read()\n",
    "    errpipe=p.stderr.read()\n",
    "    tupleP=os.waitpid(p.pid,0)\n",
    "    eC=tupleP[1]\n",
    "    return eC,pipe.decode(encoding='UTF-8'),errpipe.decode(encoding='UTF-8')\n",
    "comm=\"rucio list-rses\"\n",
    "ec,cOut,cErr = runCommand(comm)\n",
    "rses={}\n",
    "for l in cOut.split():\n",
    "    rse=str(l.strip())\n",
    "    print(rse)\n",
    "    comm=\"rucio-admin rse info \"+rse\n",
    "    ec2,cOut2,cErr2 = runCommand(comm)\n",
    "    id=None\n",
    "    for l2 in cOut2.split('\\n'):\n",
    "        if \"id: \" in l2:\n",
    "            id=l2.split()[1]\n",
    "            break\n",
    "    print(id)\n",
    "    rses[rse]=id\n",
    "with open(\"rses.pickle\", \"wb+\") as f:\n",
    "  pickle.dump(rses, f)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    with open(\"rses.pickle\", \"rb+\") as f:\n",
    "        rses = pickle.load(f)\n",
    "    return list(\n",
    "        dict(\n",
    "            [(k, v) for k, v in rses.items() if not any(tmp in k for tmp in [\"Tape\", \"Test\", \"Temp\"])]\n",
    "        ).values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58e9fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/awg/cms/rucio/2021-08-30/replicas/ exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Prepare Spark dataframes using DIDS, REPLICAS AND CONTENTS Rucio tables in HDFS\n",
    "\n",
    "    --- DBS terminology is used ---\n",
    "    - file:    file in Rucio\n",
    "    - block:   dataset in Rucio\n",
    "    - dataset: container in Rucio\n",
    "\"\"\"\n",
    "HDFS_RUCIO_REPLICAS = get_hdfs_rucio_replicas_path(spark)\n",
    "# STEP-1: Get file to block mappings from CONTENTS table as spark df\n",
    "df_contents_f_to_b = spark.read.format(\"com.databricks.spark.avro\").load(HDFS_RUCIO_CONTENTS) \\\n",
    "    .withColumnRenamed(\"NAME\", \"block\") \\\n",
    "    .withColumnRenamed(\"CHILD_NAME\", \"file\") \\\n",
    "    .select([\"block\", \"file\"]) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4dfc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-2: Get block to dataset mappings from CONTENTS table as spark df\n",
    "df_contents_b_to_d = spark.read.format(\"com.databricks.spark.avro\").load(HDFS_RUCIO_CONTENTS) \\\n",
    "    .withColumnRenamed(\"NAME\", \"dataset\") \\\n",
    "    .withColumnRenamed(\"CHILD_NAME\", \"block\") \\\n",
    "    .select([\"dataset\", \"block\"]) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be544bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-3: Get ids of only Disk RSEs\n",
    "disk_rse_ids = get_disk_rse_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e787e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-4: Create spark dataframe for REPLICAS table by filtering only Disk RSEs of CMS. Importance:\n",
    "#           - provides files in Disk RSEs\n",
    "#           - provides file size in RSEs (not all of them, see Step-6)\n",
    "df_replicas = spark.read.format(\"avro\") \\\n",
    "    .load(HDFS_RUCIO_REPLICAS) \\\n",
    "    .withColumn(\"rse_id\", lower(_hex(col(\"RSE_ID\")))) \\\n",
    "    .withColumn(\"fsize_replicas\", col(\"BYTES\").cast(LongType())) \\\n",
    "    .withColumnRenamed(\"NAME\", \"file\") \\\n",
    "    .filter(col(\"rse_id\").isin(disk_rse_ids)) \\\n",
    "    .filter(col(\"SCOPE\") == \"cms\") \\\n",
    "    .select([\"file\", \"rse_id\", \"fsize_replicas\"]) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c688f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-5: Create spark dataframe for DIDS table by selecting only Files. Importance:\n",
    "#           - provides whole files in CMS\n",
    "#           - provides file access times!\n",
    "#           - provides file size (compatible with replicas, tested), (not all of them, see step-6)\n",
    "df_dids_files = spark.read.format(\"avro\") \\\n",
    "    .load(HDFS_RUCIO_DIDS) \\\n",
    "    .filter(col(\"SCOPE\") == \"cms\") \\\n",
    "    .filter(col(\"DID_TYPE\") == \"F\") \\\n",
    "    .withColumnRenamed(\"NAME\", \"file\") \\\n",
    "    .withColumnRenamed(\"ACCESSED_AT\", \"accessed_at\") \\\n",
    "    .withColumn(\"fsize_dids\", col(\"BYTES\").cast(LongType())) \\\n",
    "    .select([\"file\", \"fsize_dids\", \"accessed_at\"]) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd39222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-6: Left join df_replicas and df_dids_files to fill the fsize for all files. Importance:\n",
    "#           - fills fsize for all files by combining both REPLICAS values and DIDS values\n",
    "df_replicas_j_dids = df_replicas.join(df_dids_files, [\"file\"], how=\"left\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca17e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-7: Check that REPLICAS and DIDS Files join filled the file size values of all files. Yes!\n",
    "if df_replicas_j_dids.filter(col(\"fsize_dids\").isNull() & col(\"fsize_replicas\").isNull()).head():\n",
    "    print(\"We have a problem! At least one of them should not be null !\")\n",
    "    sys.exit(1)\n",
    "# STEP-8: Check that REPLICAS and DIDS Files size values are compatible. Yes!\n",
    "elif df_replicas_j_dids.withColumn(\"bytes_ratio\",\n",
    "                               when(\n",
    "                                   col(\"fsize_dids\").isNotNull() & col(\"fsize_replicas\").isNotNull(),\n",
    "                                   col(\"fsize_dids\") / col(\"fsize_replicas\")\n",
    "                               ).otherwise(\"0\")\n",
    "                               ).filter((col(\"bytes_ratio\") != 1.0) & (col(\"bytes_ratio\") != 0)).head():\n",
    "    print(\"We have a problem, bytes are not equal in DIDS and REPLICAS!\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0262b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-9: fsize_dids or fsize_replicas should not be null. Just combine them to fill file sizes.\n",
    "#   - Because size of files are filled, I called \"complete\"\n",
    "df_files_complete = df_replicas_j_dids \\\n",
    "    .withColumn(\"fsize\",\n",
    "                when(col(\"fsize_dids\").isNotNull(), col(\"fsize_dids\"))\n",
    "                .when(col(\"fsize_replicas\").isNotNull(), col(\"fsize_replicas\"))\n",
    "                ) \\\n",
    "    .select(['file', 'rse_id', 'accessed_at', 'fsize']) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d856c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# Continue with joins\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# --- STEP-10 / Tests to check dataframes are okay ---:\n",
    "#         df_block_file_rse.select(\"file\").distinct().count() =  is 29921156\n",
    "#         df_block_file_rse.filter(col(\"file\").isNull()).count() = 0\n",
    "#         df_block_file_rse.filter(col(\"block\").isNull()).count() = 57892\n",
    "#         Above line means, we cannot extract block names of 57892 file from CONTENTS table ..\n",
    "#         .. which provides F:D and D:C mapping (file, dataset, container in Rucio terms)\n",
    "#         df_block_file_rse.filter(col(\"rse_id\").isNull()).count() = 0\n",
    "#         df_block_file_rse.filter(col(\"fsize\").isNull()).count() = 0\n",
    "#         We are all good, just drop null block names.\n",
    "\n",
    "# STEP-10: Left join df_files_complete and df_contents_f_to_b to get block names of files.\n",
    "#   - There are some files that we cannot extract their block names from CONTENTS table\n",
    "#   - So filter out them.\n",
    "df_block_file_rse = df_files_complete \\\n",
    "    .join(df_contents_f_to_b, [\"file\"], how=\"left\") \\\n",
    "    .select(['block', 'file', 'rse_id', 'accessed_at', 'fsize', ]) \\\n",
    "    .filter(col(\"block\").isNotNull()) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e40ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP-11 / Tests to check dataframes are okay ---:\n",
    "#         df_all.filter(col(\"dataset\").isNull()).count() = 280821\n",
    "\n",
    "# STEP-11: Left join df_block_file_rse and df_contents_b_to_d to get dataset names of blocks&files.\n",
    "#   - There are some blocks that we cannot extract their dataset names from CONTENTS table.\n",
    "#   - So filter out them.\n",
    "df_all = df_block_file_rse \\\n",
    "    .join(df_contents_b_to_d, [\"block\"], how=\"left\") \\\n",
    "    .select(['dataset', 'block', 'file', 'rse_id', 'accessed_at', 'fsize']) \\\n",
    "    .filter(col(\"dataset\").isNotNull()) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc57987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-12: Group by \"dataset\" and \"rses\" to calculate:\n",
    "#       - dataset_size_in_rse: total size of dataset in a RSE by summing up dataset's all files in that RSE.\n",
    "#       - `last_access_time_of_dataset_per_rse`: last access time of dataset in a RSE ...\n",
    "#           ... by getting max of file `accessed_at` field of dataset's all files in that RSE.\n",
    "#       - `#files_null_access_time_per_rse`: number of files which has NULL `accessed_at` field ...\n",
    "#           ... in each dataset in a RSE. ...\n",
    "#           ... This important to know to filter out if there is any NULL accessed_at file in calculation.\n",
    "#       - `#files_per_rse`: number of files od the dataset in that RSE\n",
    "#       - `#files_unique_per_rse`: unique count of dataset files in that RSE\n",
    "#       Final result will be like: one dataset can be in multiple RSEs and presumably ...\n",
    "#           ... it may have different sizes since a dataset may lost one of its block or file in a RSE?\n",
    "df_final_dataset_rse = df_all \\\n",
    "    .groupby([\"dataset\", \"rse_id\"]) \\\n",
    "    .agg(_sum(col(\"fsize\")).alias(\"dataset_size_in_rse\"),\n",
    "         _max(col(\"accessed_at\")).alias(\"last_access_time_of_dataset_per_rse\"),\n",
    "         _sum(when(col(\"accessed_at\").isNull(), 1).otherwise(0)).alias(\"#files_null_access_time_per_rse\"),\n",
    "         _count(lit(1)).alias(\"#files_per_rse\"),\n",
    "         countDistinct(col(\"file\")).alias(\"#files_unique_per_rse\"),\n",
    "         ) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcd80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-13: Get thresholds. They are unix timestamps which are 3, 6 and 12 months ago from today.\n",
    "ts_thresholds = get_ts_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ca97070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-14:\n",
    "#   Filter for calculating last_accessed_at_least_{12|6|3}_months_ago columns.\n",
    "#       - To produce correct results, \"last_access_time_of_dataset_per_rse\" field should not be null\n",
    "#           which means a dataset's all files' accessed_at fields are filled.\n",
    "#       - And \"#files_null_access_time_per_rse\"==0 means that there should not be ...\n",
    "#           any file with NULL \"accessed_at\" field.\n",
    "# Group by dataset to get final result from all RSEs' datasets.\n",
    "#   - max_dataset_size(TB): max size of dataset in all RSEs that contain this dataset\n",
    "#   - max_dataset_size(TB): min size of dataset in all RSEs that contain this dataset\n",
    "#   - max_dataset_size(TB): avg size of dataset in all RSEs that contain this dataset\n",
    "#   - last_access_time_of_dataset: last access time of dataset in all RSEs\n",
    "df = df_final_dataset_rse \\\n",
    "    .filter(col(\"last_access_time_of_dataset_per_rse\").isNotNull() &\n",
    "            (col(\"#files_null_access_time_per_rse\") == 0)\n",
    "            ) \\\n",
    "    .groupby([\"dataset\"]) \\\n",
    "    .agg(_round(_max(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"max_dataset_size(TB)\"),\n",
    "         _round(_min(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"min_dataset_size(TB)\"),\n",
    "         _round(_avg(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"avg_dataset_size(TB)\"),\n",
    "         _sum(col(\"#files_null_access_time_per_rse\")).alias(\"#files_null_access_time_per_dataset\"),\n",
    "         _max(col(\"last_access_time_of_dataset_per_rse\")).alias(\"last_access_time_of_dataset\"),\n",
    "         ) \\\n",
    "    .withColumn('last_access_more_than_12_months_ago',\n",
    "                when(col('last_access_time_of_dataset') < ts_thresholds[12], 1).otherwise(0)\n",
    "                ) \\\n",
    "    .withColumn('last_access_more_than_6_months_ago',\n",
    "                when(col('last_access_time_of_dataset') < ts_thresholds[6], 1).otherwise(0)\n",
    "                ) \\\n",
    "    .withColumn('last_access_more_than_3_months_ago',\n",
    "                when(col('last_access_time_of_dataset') < ts_thresholds[3], 1).otherwise(0)\n",
    "                ) \\\n",
    "    .filter((col('last_access_more_than_12_months_ago') == 1) |\n",
    "            (col('last_access_more_than_6_months_ago') == 1) |\n",
    "            (col('last_access_more_than_3_months_ago') == 1)\n",
    "            ) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091e40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-15: Find datasets which have only null accessed_at fields in its files\n",
    "df_all_null_accessed_at = df_final_dataset_rse \\\n",
    "    .filter(col(\"last_access_time_of_dataset_per_rse\").isNull()) \\\n",
    "    .groupby([\"dataset\"]) \\\n",
    "    .agg(_round(_max(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"max_dataset_size(TB)\"),\n",
    "         _round(_min(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"min_dataset_size(TB)\"),\n",
    "         _round(_avg(col(\"dataset_size_in_rse\")) / (10 ** 12), 2).alias(\"avg_dataset_size(TB)\"),\n",
    "         _sum(col(\"#files_null_access_time_per_rse\")).alias(\"#files_null_access_time_per_dataset\"),\n",
    "         _max(col(\"last_access_time_of_dataset_per_rse\")).alias(\"last_access_time_of_dataset\"),\n",
    "         ) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c417c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Total for not null data: not read more than 3,6,12 months which is equal to more than 3 months values.\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|sum(max_dataset_size(TB))|sum(min_dataset_size(TB))|sum(avg_dataset_size(TB))|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|        7064.480000000004|        5127.929999999998|        5802.349999999998|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Total for not null data: not read more than 3,6,12 months which is equal to more than 3 months values.\")\n",
    "df.select([\"max_dataset_size(TB)\", \"min_dataset_size(TB)\", \"avg_dataset_size(TB)\"]).groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f5e22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# For 12 months\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|sum(max_dataset_size(TB))|sum(min_dataset_size(TB))|sum(avg_dataset_size(TB))|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|       1916.1899999999994|       1817.5399999999988|        1854.879999999998|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70836"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# For 12 months\")\n",
    "df.filter(col(\"last_access_more_than_12_months_ago\") == 1).select(\n",
    "    [\"max_dataset_size(TB)\", \"min_dataset_size(TB)\", \"avg_dataset_size(TB)\"]).groupBy().sum().show()\n",
    "df.filter(col(\"last_access_more_than_12_months_ago\") == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed8f6095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# For 6 months\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|sum(max_dataset_size(TB))|sum(min_dataset_size(TB))|sum(avg_dataset_size(TB))|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|        6888.910000000002|        5045.379999999996|        5698.329999999999|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109068"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# For 6 months\")\n",
    "df.filter(col(\"last_access_more_than_6_months_ago\") == 1).select(\n",
    "    [\"max_dataset_size(TB)\", \"min_dataset_size(TB)\", \"avg_dataset_size(TB)\"]).groupBy().sum().show()\n",
    "df.filter(col(\"last_access_more_than_6_months_ago\") == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a59ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# For 3 months\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|sum(max_dataset_size(TB))|sum(min_dataset_size(TB))|sum(avg_dataset_size(TB))|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|        7064.480000000001|        5127.929999999998|        5802.349999999998|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110092"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# For 3 months\")\n",
    "df.filter(col(\"last_access_more_than_3_months_ago\") == 1).select(\n",
    "    [\"max_dataset_size(TB)\", \"min_dataset_size(TB)\", \"avg_dataset_size(TB)\"]).groupBy().sum().show()\n",
    "df.filter(col(\"last_access_more_than_3_months_ago\") == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ab8ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# For all null accessed_at(all files) datasets\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|sum(max_dataset_size(TB))|sum(min_dataset_size(TB))|sum(avg_dataset_size(TB))|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|        34172.88999999998|        19165.25000000001|       21979.240000000016|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262245"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# For all null accessed_at(all files) datasets\")\n",
    "df_all_null_accessed_at.select(\n",
    "    [\"max_dataset_size(TB)\", \"min_dataset_size(TB)\", \"avg_dataset_size(TB)\"]).groupBy().sum().show()\n",
    "df_all_null_accessed_at.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e439b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91536c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56de825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2c001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c6821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.0.1"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
